from datetime import datetime
import subprocess
import json
import os
import logging
from typing import List, Dict, Optional
from .database import db
from .utils.response import handle_exception
from .utils.constants import EXIFTOOL_PATH, GROUP_TIME_LIMIT

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def parse_exif_data_batch(image_paths: List[str]) -> List[Dict]:
    """ExifToolì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì´ë¯¸ì§€ì˜ EXIF ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¶”ì¶œ"""
    try:
        # Windows ê²½ë¡œë¥¼ ì •ê·œí™”
        normalized_paths = [os.path.normpath(path) for path in image_paths]
        logger.info(f"Processing images with paths: {normalized_paths}")  # ë””ë²„ê¹…ìš© ë¡œê·¸
        
        result = subprocess.run(
            [EXIFTOOL_PATH, "-j"] + normalized_paths,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=30
        )
        
        if result.stderr:
            logger.warning(f"ExifTool warnings: {result.stderr}")
            
        if not result.stdout.strip():
            logger.warning("No EXIF data returned for batch processing")
            return []
            
        metadata_list = json.loads(result.stdout)
        return metadata_list
    except subprocess.TimeoutExpired:
        logger.error("ExifTool process timed out")
        return []
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse ExifTool output: {e}")
        return []
    except Exception as e:
        logger.error(f"Error in batch EXIF data parsing: {str(e)}")
        return []
def convert_gps_to_decimal(gps_data, ref):
    """
    EXIF GPS ë°ì´í„°ë¥¼ ì‹­ì§„ë²•(Decimal Degrees)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    gps_data: [ë„, ë¶„, ì´ˆ] í˜•ì‹
    ref: 'N', 'S', 'E', 'W' ë°©í–¥
    """
    try:
        degrees = float(gps_data[0])
        minutes = float(gps_data[1]) / 60
        seconds = float(gps_data[2]) / 3600
        decimal = degrees + minutes + seconds

        # ë‚¨ë°˜êµ¬(S) ë˜ëŠ” ì„œê²½(W)ì¼ ê²½ìš° ìŒìˆ˜ ì²˜ë¦¬
        if ref in ['S', 'W']:
            decimal *= -1

        return round(decimal, 6)  # ì†Œìˆ˜ì  6ìë¦¬ê¹Œì§€ ë³€í™˜
    except Exception as e:
        logger.error(f"Error converting GPS data: {str(e)}")
        return None

def validate_project_info(project_info: Dict) -> bool:
    """í”„ë¡œì íŠ¸ ì •ë³´ ìœ íš¨ì„± ê²€ì‚¬"""
    required_fields = ['name', 'id']
    return all(field in project_info for field in required_fields)

from datetime import datetime

def create_exif_data(metadata: Dict, image_path: str, project_info: Dict, 
                    analysis_folder: str, session_id: str) -> Optional[Dict]:
    """EXIF ë©”íƒ€ë°ì´í„°ë¡œë¶€í„° êµ¬ì¡°í™”ëœ ë°ì´í„° ìƒì„±"""
    try:
        serial_number = metadata.get("SerialNumber", "UNKNOWN")
        date_time = metadata.get("DateTimeOriginal")

        if date_time:
            try:
                # EXIF ë‚ ì§œ í¬ë§· ë³€í™˜
                date_obj = datetime.strptime(date_time, '%Y:%m:%d %H:%M:%S')
            except ValueError:
                logger.warning(f"Invalid date format in {image_path}, using current time")
                date_obj = datetime.utcnow()
        else:
            date_obj = datetime.utcnow()
            logger.warning(f"No DateTimeOriginal found for {image_path}, using current time")

        # MongoDBì— ISODate í˜•ì‹ìœ¼ë¡œ ì €ì¥
        date_time_mongo = {"$date": date_obj.isoformat() + "Z"}

        # ìœ„ë„/ê²½ë„ ì¶”ì¶œ
        latitude = metadata.get("GPSLatitude")
        longitude = metadata.get("GPSLongitude")

        # ìœ„ë„/ê²½ë„ ê°’ì´ ìˆì„ ë•Œë§Œ ë³€í™˜
        if latitude and longitude:
            try:
                latitude = convert_gps_to_decimal(latitude, metadata.get("GPSLatitudeRef", "N"))
                longitude = convert_gps_to_decimal(longitude, metadata.get("GPSLongitudeRef", "E"))
            except Exception as e:
                logger.warning(f"Failed to convert GPS coordinates for {image_path}: {e}")
                latitude = None
                longitude = None
        else:
            latitude = None
            longitude = None

        #  project_infoì—ì„œ "ID" í‚¤ í™•ì¸
        project_id = project_info.get("ID") or project_info.get("id")
        if not project_id:
            logger.error(f"Project ID missing in project_info: {project_info}")
            return None

        # íŒŒì¼ëª… ìƒì„±
        base_name, ext = os.path.splitext(os.path.basename(image_path))  
        filename = f"{date_obj.strftime('%Y%m%d-%H%M%S')}s1{ext}"  
        thumbnail_filename = f"thum_{filename}"  

        # ê²½ë¡œ ë³€í™˜ (Flask í˜¸í™˜)
        file_path = f"/mnt/{project_id}/{analysis_folder}/source/{filename}".replace("\\", "/")
        thumbnail_path = f"/mnt/{project_id}/{analysis_folder}/thumbnail/{thumbnail_filename}".replace("\\", "/")

        return {
            "FileName": filename,  
            "FilePath": file_path,
            "OriginalFileName": os.path.basename(image_path),
            "ThumnailPath": thumbnail_path,
            "SerialNumber": serial_number,
            "UserLabel": metadata.get("UserLabel", "UNKNOWN"),
            "DateTimeOriginal": date_time_mongo,
            "Latitude": latitude,  
            "Longitude": longitude,  
            
            "ProjectInfo": {
                "ProjectName": project_info.get("ProjectName", "Unknown"),
                "ID": project_id  
            },
            "AnalysisFolder": analysis_folder,
            "sessionid": [session_id],
            "uploadState": "uploaded",
            "serial_filename": f"{serial_number}_{filename}", 
            "__v": 0
        }
    except Exception as e:
        logger.error(f"Error creating EXIF data structure: {str(e)}")
        return None
    
def get_all_evtnum_first_images(project_id: str, serial_number: str) -> List[Dict]:
    """
    í•´ë‹¹ í”„ë¡œì íŠ¸ + SerialNumberì—ì„œ ê° evtnum ê·¸ë£¹ì˜ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì˜´.
    """
    pipeline = [
        {"$match": {"ProjectInfo.ID": project_id, "SerialNumber": serial_number}},  # SerialNumber í•„í„° ì ìš©
        {"$sort": {"evtnum": 1, "DateTimeOriginal": 1}},
        {"$group": {
            "_id": "$evtnum",
            "first_image": {"$first": "$$ROOT"}
        }},
        {"$replaceRoot": {"newRoot": "$first_image"}}
    ]
    return list(db.images.aggregate(pipeline))


def assign_evtnum_to_group(images: List[Dict], project_id: str, serial_number: str) -> List[Dict]:
    """
    ìƒˆë¡œìš´ ì´ë¯¸ì§€ ê·¸ë£¹ì— evtnum í• ë‹¹í•  ë•Œ, ê¸°ì¡´ ê·¸ë£¹ì—ì„œ ê°€ì¥ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì™€ ë¹„êµí•˜ì—¬ 5ë¶„ ì´ˆê³¼ ì—¬ë¶€ ê²°ì •.
    """
    if not images:
        return []

    # ê¸°ì¡´ DBì—ì„œ í•´ë‹¹ SerialNumberì˜ ëª¨ë“  evtnum ì²« ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
    evtnum_first_images = get_all_evtnum_first_images(project_id, serial_number)

    # ìƒˆë¡œ ì—…ë¡œë“œí•œ ì´ë¯¸ì§€ ì¤‘ ê°€ì¥ ì²« ë²ˆì§¸ ì´¬ì˜ëœ ì´ë¯¸ì§€ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
    sorted_images = sorted(images, key=lambda x: x["DateTimeOriginal"]["$date"])
    first_image_time = datetime.fromisoformat(sorted_images[0]["DateTimeOriginal"]["$date"].replace("Z", ""))

    new_evtnum = None

    if evtnum_first_images:
        for evtnum_data in evtnum_first_images:
            existing_time = datetime.fromisoformat(evtnum_data["DateTimeOriginal"]["$date"].replace("Z", ""))

            # ë‚ ì§œ(ì—°-ì›”-ì¼)ê°€ ë‹¤ë¥´ë©´ ë‹¤ë¥¸ ê·¸ë£¹ì´ì–´ì•¼ í•¨
            if existing_time.date() != first_image_time.date():
                continue  

            # 5ë¶„ ì´ë‚´ë¼ë©´ ê°™ì€ evtnum ì‚¬ìš©
            if (first_image_time - existing_time).total_seconds() / 60 <= GROUP_TIME_LIMIT:
                new_evtnum = evtnum_data.get("evtnum")  
                break  

    # ê¸°ì¡´ ê·¸ë£¹ê³¼ ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œìš´ evtnum í• ë‹¹ (í”„ë¡œì íŠ¸ ë‚´ì—ì„œ ê°€ì¥ í° ê°’ +1)
    if new_evtnum is None:
        new_evtnum = get_next_evtnum(project_id)

    # ì´ë¯¸ì§€ë“¤ì— evtnum í• ë‹¹
    for img in images:
        img["evtnum"] = new_evtnum

    return images

def get_next_evtnum(project_id: str) -> int:
    """
    í•´ë‹¹ í”„ë¡œì íŠ¸ì—ì„œ ê°€ì¥ í° evtnumì„ ì°¾ì•„ +1ì„ ë°˜í™˜.
    """
    last_entry = db.images.find_one(
        {"ProjectInfo.ID": project_id},  # ğŸ” SerialNumber ê³ ë ¤ X, í”„ë¡œì íŠ¸ ì „ì²´ ê¸°ì¤€
        sort=[("evtnum", -1)]  # evtnumì´ ê°€ì¥ í° ê°’ì„ ê°€ì ¸ì˜´
    )
    last_evtnum = last_entry.get("evtnum", 0) if last_entry else 0  # ì—†ìœ¼ë©´ 0ë¶€í„° ì‹œì‘
    return last_evtnum + 1  # í•­ìƒ í”„ë¡œì íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ìœ ì¼í•œ evtnum ë³´ì¥



GROUP_TIME_LIMIT = 5  #  5ë¶„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”

def group_images_by_time(image_list: List[Dict], project_id: str) -> List[Dict]:
    """
    í”„ë¡œì íŠ¸ ID + SerialNumber ê¸°ì¤€ìœ¼ë¡œ ì‹œê°„ë³„ ê·¸ë£¹í™”í•˜ê³  evtnum í• ë‹¹.
    """
    if not image_list:
        return []

    result = []
    grouped_by_project_serial = {}

    # ê°™ì€ í”„ë¡œì íŠ¸, ê°™ì€ SerialNumber ë‚´ì—ì„œ ê·¸ë£¹í•‘
    for img in image_list:
        key = f"{img['ProjectInfo']['ID']}_{img['SerialNumber']}"
        grouped_by_project_serial.setdefault(key, []).append(img)

    for key, group in grouped_by_project_serial.items():
        project_id, serial_number = key.split("_")

        # ê¸°ì¡´ DBì˜ ëª¨ë“  evtnum ê·¸ë£¹ì˜ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        evtnum_info_list = get_all_evtnum_first_images(project_id, serial_number)

        # ìƒˆë¡œìš´ ì—…ë¡œë“œ ì´ë¯¸ì§€ ì •ë ¬
        sorted_group = sorted(group, key=lambda x: x['DateTimeOriginal']['$date'])
        
        logger.info(f"Processing SerialNumber: {serial_number}, Total Images: {len(sorted_group)}")
        logger.info(f"Existing evtnum groups: {evtnum_info_list}")

        # evtnum í• ë‹¹
        assigned_group = assign_evtnum_to_group(sorted_group, project_id, serial_number)
        
        # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€: evtnumì´ ì •ìƒì ìœ¼ë¡œ ë¶€ì—¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
        for img in assigned_group:
            logger.info(f"Image: {img['OriginalFileName']} -> evtnum: {img.get('evtnum')}")

        result.extend(assigned_group)

    return result

def batch_processing(image_paths: List[str], batch_size: int = 100) -> List[List[str]]:
    """ì´ë¯¸ì§€ë¥¼ batch_size ê°œìˆ˜ë§Œí¼ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜"""
    return [image_paths[i:i + batch_size] for i in range(0, len(image_paths), batch_size)]

def process_images(image_paths: List[str], project_info: Dict, 
                  analysis_folder: str, session_id: str) -> List[Dict]:
    """ì´ë¯¸ì§€ë¥¼ 100ê°œì”© ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•˜ê³  MongoDB ì €ì¥ìš© ë°ì´í„° ìƒì„±"""
    try:
        if not image_paths:
            logger.warning("âš  No images provided for processing")
            return []

        if not validate_project_info(project_info):
            raise ValueError("Invalid project_info structure")

        grouped_images = []  # ìµœì¢… ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        # 100ê°œì”© ë‚˜ëˆ„ì–´ ë°°ì¹˜ ì²˜ë¦¬
        image_batches = batch_processing(image_paths, batch_size=100)
        for batch_idx, image_batch in enumerate(image_batches):
            logger.info(f"Processing batch {batch_idx + 1}/{len(image_batches)} with {len(image_batch)} images")

            #  EXIF ë°ì´í„° ì¶”ì¶œ
            metadata_list = parse_exif_data_batch(image_batch)
            if not metadata_list:
                logger.error(f" No EXIF data could be extracted from batch {batch_idx + 1}")
                continue

            #  EXIF ë°ì´í„° êµ¬ì¡°í™”
            image_data_list = []
            for metadata, image_path in zip(metadata_list, image_batch):
                logger.info(f" Processing Image: {image_path}")

                exif_data = create_exif_data(
                    metadata, image_path, project_info, analysis_folder, session_id
                )
                if not exif_data:
                    logger.error(f" Failed to create EXIF data for {image_path}")
                    continue

                # ì¶”ê°€ ë¡œê·¸: DateTimeOriginal í•„ë“œ í™•ì¸
                if "DateTimeOriginal" not in exif_data:
                    logger.error(f"DateTimeOriginal í‚¤ê°€ ì—†ìŒ! {exif_data}")
                else:
                    logger.info(f"âœ” DateTimeOriginal í™•ì¸: {exif_data['DateTimeOriginal']}")

                logger.info(f" EXIF Data Created: {exif_data}")
                image_data_list.append(exif_data)

            #  ì‹œê°„ë³„ ê·¸ë£¹í™”
            for img in image_data_list:
                logger.info(f" DateTimeOriginal: {img['DateTimeOriginal']}")

            batch_grouped_images = group_images_by_time(image_data_list, project_info["id"])
            grouped_images.extend(batch_grouped_images)  # ì „ì²´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

        logger.info(f" Successfully processed {len(grouped_images)} images in total")
        return grouped_images

    except Exception as e:
        logger.error(f" Error in process_images: {str(e)}")
        return []